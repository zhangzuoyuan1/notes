关键词：
	同时理解时间上下文和变量之间的相互关系
	自监督模型
	数据退化：四种类型的合成异常值
		方案：其中一部分输入数据被其中一个合成异常值替换
	自注意力机制：基于transformer的架构
	多变量数据点转换为具有**相对位置偏差**的时间表示，从中产生异常分数
结论：
	1D相对位置嵌入
	可学习区分异常行为
	复杂时间序列真实世界中效果好
	未来方向：
		修改退化算法
		根据数据特征混合适当类型的异常值
		分析异常值合成过程
数据退化算法or方案の解释：
	通过替换输入数据的随机部分并训练模型来找到降级部分来修改掩码语言建模（MLM）
相关工作：
	局部异常因子LOF 基于密度的异常检测 2000
	基于ML（机器学习）的方法 2008
	高斯混合模型GMM与深度神经网络相结合 2018
	RNN处理复杂的时间序列 2018
	LSTM-VAE 变分自编码器模型，前馈网络被LSTM替代 2018
	GRU门控循环单元+VAE 2019
	图神经网络GNN 2021
	基于transformer 2021
	 BERT引入跨度掩码而不是MLM+自编码和自回归 2019 
	VIT 采用无CNN架构的transformer编码器
模型框架：
![[Pasted image 20241031142437.png]]
MLP：多层感知机：作为transformer中的前馈神经网络FF出现
LN：layerNorm归一化层
MSA：多头自注意力机制
清晰图二：
![[Pasted image 20241031142814.png]]
与原始 Transform 或 ViT 不同，我们不使用正弦位置编码（Vaswani 等人，2017 年）或绝对位置嵌入（Dosovitskiy 等人，2020 年）来注入位置信息。我们改为在每个注意力矩阵中添加 1D 相对位置偏差（Raffel 等人，2019 年；Liu 等人，2021 年），以考虑窗口中特征之间的相对位置。
![[Pasted image 20241031143048.png]]
数据退化合成异常值的方式：
![[Pasted image 20241031143331.png]]
![[Pasted image 20241031145227.png]]
![[Pasted image 20241031145339.png]]
消融实验：
	均匀替换和峰值噪声补充了软替换，长度调整根据数据集有不同的影响
代码：
前馈神经网络的作用：
	前馈神经网络（Feed-Forward Neural Network, FFN）在Transformer模型中起着重要的作用。它主要用于处理每个位置的特征，增强模型的非线性能力，从而更好地捕捉输入序列中的复杂模式。以下是FFN在Transformer模型中的几个主要作用：
	1. **增强非线性能力**
	**非线性变换**：FFN通过引入非线性激活函数（如ReLU）来增加模型的非线性能力。这使得模型能够学习更复杂的特征表示，从而更好地捕捉输入数据中的非线性关系。
	**特征映射**：FFN通常包含两个线性层，中间夹一个非线性激活函数。第一个线性层将输入特征映射到一个更高的维度，第二个线性层再将高维特征映射回原始维度。这种结构有助于模型在不同的特征空间中进行有效的特征变换。
	2. **特征重组**
	**特征增强**：FFN可以对每个位置的特征进行重新组合和增强，从而提取出更有意义的特征表示。这对于后续的注意力机制和模型的其他部分是非常有益的。
	**特征融合**：通过FFN，模型可以在局部范围内对特征进行融合和重组，从而捕捉到输入序列中的局部依赖关系。
	3. **缓解梯度消失/爆炸问题**
	**残差连接**：在Transformer模型中，FFN通常与残差连接（Residual Connection）一起使用。残差连接可以缓解梯度消失和梯度爆炸问题，使模型更容易训练。
	**归一化**：FFN前后通常会添加层归一化（Layer Normalization），进一步稳定训练过程，帮助模型更快地收敛。
	4. **增加模型容量**
	**模型深度**：通过在每个Transformer块中添加FFN，可以增加模型的深度，从而提高模型的表达能力。更深的模型通常能够学习更复杂的模式和结构。
	**模型宽度**：FFN中的隐藏层通常具有较大的维度（如2048），这增加了模型的宽度，使得模型能够处理更多的特征信息。
感受：
	transformer在序列预测中的简单应用
	不是长序列
	简单的模型
	训练模式自动生成异常点以拟合检测异常状态是好的
	一坨，真一坨